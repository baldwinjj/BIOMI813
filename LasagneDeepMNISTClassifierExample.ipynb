{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 5103)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import struct\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "import lasagne.layers as L\n",
    "import lasagne.regularization as R\n",
    "import lasagne.nonlinearities as NL\n",
    "import lasagne.objectives as O\n",
    "import lasagne.init as I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0) Load MNIST digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LoadMNIST(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        header = bytearray(f.read(4)) #read the header bytes\n",
    "        datatype = 'int32'  \n",
    "        typecode = header[2] # read the type byte\n",
    "        if(typecode==8):\n",
    "            datatype = '>u1';\n",
    "        elif(typecode == 12):\n",
    "            datatype = '>f4'\n",
    "        elif(typecode == 14):\n",
    "            datatype = '>f8'    \n",
    "        print datatype\n",
    "        \n",
    "        size = np.fromfile(f, '>i4', header[3]) #read the dimensions of the array  \n",
    "        dshape = ()\n",
    "        count = 1\n",
    "        for d in range(0,len(size)):\n",
    "            count *= size[d]\n",
    "            dshape += (size[d], )        \n",
    "        print dshape\n",
    "        data = np.fromfile(f, datatype, count) #read the array data\n",
    "        return data.reshape(dshape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_trn_img = '/home/styvesg/Documents/PostDoc/Programs/JupyterNotebooks/DataSet/train-images.idx3-ubyte'\n",
    "mnist_trn_lab = '/home/styvesg/Documents/PostDoc/Programs/JupyterNotebooks/DataSet/train-labels.idx1-ubyte'\n",
    "mnist_val_img = '/home/styvesg/Documents/PostDoc/Programs/JupyterNotebooks/DataSet/t10k-images.idx3-ubyte'\n",
    "mnist_val_lab = '/home/styvesg/Documents/PostDoc/Programs/JupyterNotebooks/DataSet/t10k-labels.idx1-ubyte'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">u1\n",
      "(60000, 28, 28)\n",
      ">u1\n",
      "(10000, 28, 28)\n",
      ">u1\n",
      "(60000,)\n",
      ">u1\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "mnist_trn_data = LoadMNIST(mnist_trn_img).astype(np.float32)[:,np.newaxis,:,:]\n",
    "mnist_val_data = LoadMNIST(mnist_val_img).astype(np.float32)[:,np.newaxis,:,:]\n",
    "mnist_trn_label = LoadMNIST(mnist_trn_lab)\n",
    "mnist_val_label = LoadMNIST(mnist_val_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(_in, *args, **kwargs):\n",
    "    return L.Conv2DLayer(_in, *args, untie_biases=False, flip_filters=True, convolution=theano.tensor.nnet.conv2d, **kwargs)\n",
    "\n",
    "def batch_norm(_in, *args, **kwargs):\n",
    "    return L.batch_norm(_in, beta=None, gamma=None, *args, **kwargs)\n",
    "\n",
    "def avg(_in, *args, **kwargs):\n",
    "    return L.Pool2DLayer(_in, *args, ignore_border=True, mode='average_exc_pad', **kwargs)\n",
    "\n",
    "def flatten(_in, **kwargs):\n",
    "    return L.FlattenLayer(_in, **kwargs)\n",
    "\n",
    "def sigmoid(_in, **kwargs):\n",
    "    return L.NonlinearityLayer(_in, nonlinearity=NL.sigmoid)\n",
    "\n",
    "def tanh(_in, **kwargs):\n",
    "    return L.NonlinearityLayer(_in, nonlinearity=NL.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_lasagne_net(_net, skipnoparam=True):\n",
    "    layers = L.get_all_layers(_net)\n",
    "    for l in layers:\n",
    "        out = l.output_shape\n",
    "        par = l.get_params()\n",
    "        if skipnoparam and len(par)==0 and l.name==None:\n",
    "            continue\n",
    "        print \"Layer\\t: %s\\nName\\t: %s\\nType\\t: %s\" % (l, l.name, type(l))\n",
    "        print \"Shape\\t: %s\" % (out,)\n",
    "        if len(par)>0:\n",
    "            print \"Params\"\n",
    "            for p in par:\n",
    "                print \"        |-- {:<10}: {:}\".format(p.name, p.get_value().shape,)\n",
    "        print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "        \n",
    "        \n",
    "def embedding(x, c=10): \n",
    "    '''SparseIntegerEmbedding'''\n",
    "    y = np.zeros((len(x), c), dtype=np.float32)\n",
    "    y[np.arange(len(x)), x] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1) Define the classifier net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npc = 1         # # of channels in image\n",
    "npx = 28        # # of pixels width/height of images\n",
    "ny = 10         # # of classes\n",
    "\n",
    "lr = 2e-3       # initial learning rate for adam\n",
    "l2 = 2.0e-3     # l2 weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_X = T.tensor4() # a theano variable representing the input data\n",
    "_Y = T.matrix()  # a theano variable representing the class label\n",
    "\n",
    "def Classifier(__X):\n",
    "    _input = L.InputLayer((None, npc, npx, npx), input_var=__X, name='X')\n",
    "    _drop1 = L.DropoutLayer(_input, p=0.2, rescale=True)\n",
    "    _conv1 = batch_norm(conv(_drop1, num_filters=64, filter_size=7, stride=3, pad=0, W=I.Normal(0.02), b=None, nonlinearity=NL.rectify))  \n",
    "    _drop2 = L.DropoutLayer(_conv1, p=0.2, rescale=True)\n",
    "    _conv2 = batch_norm(conv(_drop2, num_filters=128, filter_size=3, stride=1, pad=0, W=I.Normal(0.02), b=None, nonlinearity=NL.rectify))\n",
    "    _pool2 = L.MaxPool2DLayer(_conv2, pool_size=2)     \n",
    "    _fc1 = batch_norm(L.DenseLayer(L.FlattenLayer(_pool2, outdim=2), 256, W=I.Normal(0.02), b=None, nonlinearity=NL.rectify))\n",
    "    _fc2 = L.DenseLayer(_fc1, ny, W=I.Normal(0.02), b=None, nonlinearity=NL.sigmoid) \n",
    "    return _fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer\t: <lasagne.layers.input.InputLayer object at 0x7f0a85e09ad0>\n",
      "Name\t: X\n",
      "Type\t: <class 'lasagne.layers.input.InputLayer'>\n",
      "Shape\t: (None, 1, 28, 28)\n",
      "\n",
      "\n",
      "Layer\t: <lasagne.layers.conv.Conv2DLayer object at 0x7f0a85d5dbd0>\n",
      "Name\t: None\n",
      "Type\t: <class 'lasagne.layers.conv.Conv2DLayer'>\n",
      "Shape\t: (None, 64, 8, 8)\n",
      "Params\n",
      "        |-- W         : (64, 1, 7, 7)\n",
      "\n",
      "\n",
      "Layer\t: <lasagne.layers.normalization.BatchNormLayer object at 0x7f0aa4650350>\n",
      "Name\t: None\n",
      "Type\t: <class 'lasagne.layers.normalization.BatchNormLayer'>\n",
      "Shape\t: (None, 64, 8, 8)\n",
      "Params\n",
      "        |-- mean      : (64,)\n",
      "        |-- inv_std   : (64,)\n",
      "\n",
      "\n",
      "Layer\t: <lasagne.layers.conv.Conv2DLayer object at 0x7f0adefeea50>\n",
      "Name\t: None\n",
      "Type\t: <class 'lasagne.layers.conv.Conv2DLayer'>\n",
      "Shape\t: (None, 128, 6, 6)\n",
      "Params\n",
      "        |-- W         : (128, 64, 3, 3)\n",
      "\n",
      "\n",
      "Layer\t: <lasagne.layers.normalization.BatchNormLayer object at 0x7f0adefee9d0>\n",
      "Name\t: None\n",
      "Type\t: <class 'lasagne.layers.normalization.BatchNormLayer'>\n",
      "Shape\t: (None, 128, 6, 6)\n",
      "Params\n",
      "        |-- mean      : (128,)\n",
      "        |-- inv_std   : (128,)\n",
      "\n",
      "\n",
      "Layer\t: <lasagne.layers.dense.DenseLayer object at 0x7f0adeea5b10>\n",
      "Name\t: None\n",
      "Type\t: <class 'lasagne.layers.dense.DenseLayer'>\n",
      "Shape\t: (None, 256)\n",
      "Params\n",
      "        |-- W         : (1152, 256)\n",
      "\n",
      "\n",
      "Layer\t: <lasagne.layers.normalization.BatchNormLayer object at 0x7f0adeea5cd0>\n",
      "Name\t: None\n",
      "Type\t: <class 'lasagne.layers.normalization.BatchNormLayer'>\n",
      "Shape\t: (None, 256)\n",
      "Params\n",
      "        |-- mean      : (256,)\n",
      "        |-- inv_std   : (256,)\n",
      "\n",
      "\n",
      "Layer\t: <lasagne.layers.dense.DenseLayer object at 0x7f0adef0d9d0>\n",
      "Name\t: None\n",
      "Type\t: <class 'lasagne.layers.dense.DenseLayer'>\n",
      "Shape\t: (None, 10)\n",
      "Params\n",
      "        |-- W         : (256, 10)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at the details of the network shape that we are attempting\n",
    "_cls = Classifier(_X)\n",
    "print_lasagne_net(_cls, skipnoparam=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2) Compile the theano expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPILING\n",
      "3.68 seconds to compile theano functions\n"
     ]
    }
   ],
   "source": [
    "tfX = np.float32\n",
    "lrt = theano.shared(tfX(lr))\n",
    "\n",
    "cls_params = L.get_all_params(_cls, trainable=True)\n",
    "\n",
    "_cls_reg = l2 * R.regularize_layer_params(_cls, R.l2) # regularization loss\n",
    "_cls_trn_pred = L.get_output(_cls, deterministic=False) # training prediction\n",
    "_cls_trn_loss = O.squared_error(_cls_trn_pred, _Y).mean() + _cls_reg # total training loss\n",
    "\n",
    "_cls_val_pred = L.get_output(_cls, deterministic=True) #validation prediction\n",
    "_cls_val_loss = O.squared_error(_cls_val_pred, _Y).mean() + _cls_reg # total validation loss\n",
    "_cls_val_acc = T.mean(T.eq(T.argmax(_cls_val_pred, axis=1), T.argmax(_Y, axis=1)), dtype=theano.config.floatX) # validation accuracies\n",
    "\n",
    "_class = T.extra_ops.to_one_hot(T.argmax(_cls_val_pred, axis=1), ny)\n",
    "\n",
    "cls_updates = lasagne.updates.adam(_cls_trn_loss, cls_params, learning_rate=lrt, beta1=0.5, epsilon=1e-12) \n",
    "\n",
    "print 'COMPILING'\n",
    "t = time.time()\n",
    "cls_trn_fn = theano.function([_X, _Y], [_cls_trn_loss, _cls_val_acc], updates=cls_updates)\n",
    "cls_val_fn = theano.function([_X, _Y], [_cls_val_loss, _cls_val_acc])\n",
    "cls_pred_fn = theano.function([_X], _class)\n",
    "print '%.2f seconds to compile theano functions'%(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reloading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to reload values from file\n",
    "\n",
    "# #cls_param_file = open(params_dir + cls_filename, 'rb')\n",
    "# #cls_param_value = pickle.load(cls_param_file)\n",
    "# #cls_param_file.close()\n",
    "# #L.set_all_param_values(_cls, cls_param_value)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model. \n",
    "\n",
    "-Decrease learning rate by 'perc_decay'% every epoch after 'niter' epoch at initial rate 'lr'.\n",
    "\n",
    "-Record the history of the training and validation accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [00:02, 53.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 1 of 50 took 2.333s\n",
      "  training loss:       0.028682\n",
      "  validation accuracy: 93.05 %\n",
      "  validation loss:     0.014054\n",
      "  validation accuracy: 98.04 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 52.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 2 of 50 took 2.327s\n",
      "  training loss:       0.014761\n",
      "  validation accuracy: 98.40 %\n",
      "  validation loss:     0.012355\n",
      "  validation accuracy: 98.58 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 56.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 3 of 50 took 2.178s\n",
      "  training loss:       0.013732\n",
      "  validation accuracy: 98.77 %\n",
      "  validation loss:     0.011852\n",
      "  validation accuracy: 98.95 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 54.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 4 of 50 took 2.268s\n",
      "  training loss:       0.013298\n",
      "  validation accuracy: 98.95 %\n",
      "  validation loss:     0.011632\n",
      "  validation accuracy: 99.01 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 54.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 5 of 50 took 2.277s\n",
      "  training loss:       0.012995\n",
      "  validation accuracy: 99.08 %\n",
      "  validation loss:     0.011083\n",
      "  validation accuracy: 99.04 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 54.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 6 of 50 took 2.244s\n",
      "  training loss:       0.012784\n",
      "  validation accuracy: 99.15 %\n",
      "  validation loss:     0.011124\n",
      "  validation accuracy: 99.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 54.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 7 of 50 took 2.230s\n",
      "  training loss:       0.012610\n",
      "  validation accuracy: 99.25 %\n",
      "  validation loss:     0.011171\n",
      "  validation accuracy: 98.94 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 53.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 8 of 50 took 2.202s\n",
      "  training loss:       0.012470\n",
      "  validation accuracy: 99.28 %\n",
      "  validation loss:     0.010958\n",
      "  validation accuracy: 99.18 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 57.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 9 of 50 took 2.161s\n",
      "  training loss:       0.012370\n",
      "  validation accuracy: 99.34 %\n",
      "  validation loss:     0.010956\n",
      "  validation accuracy: 99.04 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 51.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 10 of 50 took 2.386s\n",
      "  training loss:       0.012283\n",
      "  validation accuracy: 99.35 %\n",
      "  validation loss:     0.010867\n",
      "  validation accuracy: 99.02 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 52.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 11 of 50 took 2.366s\n",
      "  training loss:       0.012216\n",
      "  validation accuracy: 99.37 %\n",
      "  validation loss:     0.010701\n",
      "  validation accuracy: 99.17 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 53.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 12 of 50 took 2.310s\n",
      "  training loss:       0.012176\n",
      "  validation accuracy: 99.42 %\n",
      "  validation loss:     0.010843\n",
      "  validation accuracy: 99.20 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 53.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 13 of 50 took 2.318s\n",
      "  training loss:       0.012050\n",
      "  validation accuracy: 99.46 %\n",
      "  validation loss:     0.010617\n",
      "  validation accuracy: 99.21 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 55.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 14 of 50 took 2.220s\n",
      "  training loss:       0.011977\n",
      "  validation accuracy: 99.49 %\n",
      "  validation loss:     0.011097\n",
      "  validation accuracy: 99.13 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 51.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 15 of 50 took 2.393s\n",
      "  training loss:       0.011877\n",
      "  validation accuracy: 99.54 %\n",
      "  validation loss:     0.010496\n",
      "  validation accuracy: 99.19 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 55.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 16 of 50 took 2.205s\n",
      "  training loss:       0.011784\n",
      "  validation accuracy: 99.58 %\n",
      "  validation loss:     0.010414\n",
      "  validation accuracy: 99.22 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 56.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 17 of 50 took 2.180s\n",
      "  training loss:       0.011714\n",
      "  validation accuracy: 99.57 %\n",
      "  validation loss:     0.010506\n",
      "  validation accuracy: 99.22 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 51.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 18 of 50 took 2.220s\n",
      "  training loss:       0.011655\n",
      "  validation accuracy: 99.61 %\n",
      "  validation loss:     0.010448\n",
      "  validation accuracy: 99.18 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 51.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 19 of 50 took 2.373s\n",
      "  training loss:       0.011625\n",
      "  validation accuracy: 99.63 %\n",
      "  validation loss:     0.010566\n",
      "  validation accuracy: 99.16 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 53.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 20 of 50 took 2.285s\n",
      "  training loss:       0.011575\n",
      "  validation accuracy: 99.66 %\n",
      "  validation loss:     0.010433\n",
      "  validation accuracy: 99.26 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 55.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 21 of 50 took 2.206s\n",
      "  training loss:       0.011557\n",
      "  validation accuracy: 99.67 %\n",
      "  validation loss:     0.010490\n",
      "  validation accuracy: 99.19 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 52.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 22 of 50 took 2.339s\n",
      "  training loss:       0.011489\n",
      "  validation accuracy: 99.69 %\n",
      "  validation loss:     0.010564\n",
      "  validation accuracy: 99.18 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 54.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 23 of 50 took 2.274s\n",
      "  training loss:       0.011450\n",
      "  validation accuracy: 99.69 %\n",
      "  validation loss:     0.010448\n",
      "  validation accuracy: 99.23 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [00:02, 50.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 24 of 50 took 2.452s\n",
      "  training loss:       0.011432\n",
      "  validation accuracy: 99.71 %\n",
      "  validation loss:     0.010398\n",
      "  validation accuracy: 99.22 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "84it [00:01, 49.75it/s]"
     ]
    }
   ],
   "source": [
    "niter = 10        # # of iter at starting learning rate\n",
    "perc_decay = 10.0 # # of iter to linearly decay learning rate to zero\n",
    "num_epochs = 50\n",
    "batch_size = 500\n",
    "\n",
    "lrt.set_value(tfX(lr))\n",
    "\n",
    "trn_hist = []\n",
    "val_hist = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    trn_err = 0\n",
    "    trn_acc = 0\n",
    "    trn_batches = 0\n",
    "    start_time = time.time()\n",
    "    for xb, yb in tqdm(iterate_minibatches(mnist_trn_data, mnist_trn_label, batch_size, shuffle=True)):\n",
    "        err, acc = cls_trn_fn(xb, embedding(yb))\n",
    "        trn_err += err\n",
    "        trn_acc += acc\n",
    "        trn_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for xb, yb in iterate_minibatches(mnist_val_data, mnist_val_label, batch_size, shuffle=False):\n",
    "        err, acc = cls_val_fn(xb, embedding(yb))\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    trn_hist += [trn_acc / trn_batches,]\n",
    "    val_hist += [val_acc / val_batches,]\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"\\n  Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:       {:.6f}\".format(trn_err / trn_batches))\n",
    "    print(\"  validation accuracy: {:.2f} %\".format(trn_acc / trn_batches * 100))\n",
    "    print(\"  validation loss:     {:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy: {:.2f} %\".format(val_acc / val_batches * 100))\n",
    "    \n",
    "    if epoch > niter:\n",
    "        lrt.set_value(tfX(lrt.get_value() * (1.0 - perc_decay / 100.0)))     \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(trn_hist, 'b', label='trn', lw=2)\n",
    "plt.plot(val_hist, 'r', label='val', lw=2)\n",
    "plt.axhline(1./ny, color='k', linestyle='--', label='chance')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0.95, 1.0)\n",
    "plt.legend(bbox_to_anchor=(1., 0.3))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_dir = os.getcwd()\n",
    "cls_filename = \"cls_params.pkl\"\n",
    "\n",
    "cls_param_file = open(params_dir + cls_filename, 'wb')\n",
    "cls_param_values = L.get_all_param_values(_cls)\n",
    "pickle.dump(cls_param_values, cls_param_file)\n",
    "cls_param_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
