{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization \n",
    "\n",
    "Here we consider simple quadratic cost functions, examine their derivatives, and use gradient descent to find their minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic cost function, one dimension\n",
    "\n",
    "Consider a toy-example a cost function $L(w)$ in one dimension:\n",
    "\n",
    "$L(w) = w^2$\n",
    "\n",
    "When we optimize a model, we want to find the parameter $w$ that minimizes cost. In this case, the optimal value of the parameter is obviously $w=0$, but let's pretend we don't know that because, in general, we won't.\n",
    "\n",
    "To find the mimimum of this function, we start at some value of $w$ and then follow the derivative of the function for a small step-size $\\Delta$. The derivative in this case is:\n",
    "\n",
    "$\\frac{dL(w)}{dw} = 2w$\n",
    "\n",
    "This gives us the equation of the line that is tangent to the cost function at each point. We'll use \"quiver\" to draw segments (pink) of the tangent line at various points on $L(w)$. The length of the line segment is governed by $\\Delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##define step size and range of function evaluation\n",
    "delta = 0.9\n",
    "w = np.linspace(-10,10,num=100)\n",
    "\n",
    "##plot the cost function L(w)\n",
    "plt.plot(w, w**2, 'k')\n",
    "\n",
    "##plot a vector along the line defined by the derivative of L(w) for some values of w.\n",
    "d_color = np.array([.9,.4,.7])\n",
    "for ii in w[0:-1:5]:\n",
    "    plt.quiver(ii,ii**2,delta,2*ii*delta,angles='xy', scale_units ='xy',scale=1,pivot='middle',headwidth=0,color=d_color)\n",
    "plt.ylim([-5,100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try to reach the bottom of the cost function starting at some initial value $w_0$ and then travelling small distances in the direction of the derivative at each step. This is called \"gradient descent\". We'll see how important step size for determining how quickly you reach the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_0 = 10\n",
    "num_steps = 100\n",
    "\n",
    "##let's try a small step-size first\n",
    "delta = .01\n",
    "cost = np.zeros(num_steps)\n",
    "weight_trajectory = np.zeros(num_steps)\n",
    "\n",
    "##this is an example of a gradient descent training loop\n",
    "for ii in range(num_steps):\n",
    "    w_new = w_0-2*w_0*delta  ##<<<<-----------HERE IS WHERE WE TAKE THE GRADIENT STEP!\n",
    "    w_0 = np.copy(w_new)\n",
    "    cost[ii] = w_new**2\n",
    "    weight_trajectory[ii] = w_new\n",
    "\n",
    "##plot cost as a function of weight trajectory\n",
    "plt.title('step size = %0.2f' %(delta))\n",
    "plt.ylabel('cost function value')\n",
    "plt.xlabel('weight at each iteration of grad. descent.')\n",
    "plt.plot(weight_trajectory,cost, '-o', label = '$w_{>0}$')\n",
    "plt.plot(weight_trajectory[0],cost[0], 'ro', label='$w_0$')\n",
    "plt.plot(w, w**2, 'k', label='$L(w)$')\n",
    "plt.legend(numpoints=1, loc = 'best')\n",
    "plt.xlim([-11,11])\n",
    "plt.ylim([0,110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##now a larger delta\n",
    "w_0 = -10\n",
    "delta = .9\n",
    "cost = np.zeros(num_steps)\n",
    "weight_trajectory = np.zeros(num_steps)\n",
    "for ii in range(num_steps):\n",
    "    w_new = w_0-2*w_0*delta\n",
    "    w_0 = np.copy(w_new)\n",
    "    cost[ii] = w_new**2\n",
    "    weight_trajectory[ii] = w_new\n",
    "\n",
    "##plot cost as a function of weight trajectory\n",
    "plt.title('step size = %0.2f' %(delta))\n",
    "plt.ylabel('cost function value L(w)')\n",
    "plt.xlabel('weight at each iteration of grad. descent.')\n",
    "plt.plot(weight_trajectory,cost, '-o', label = '$w_{>0}$')\n",
    "plt.plot(weight_trajectory[0],cost[0], 'ro', label='$w_0$')\n",
    "plt.plot(w, w**2, 'k', label='$L(w)$')\n",
    "plt.legend(numpoints=1, loc = 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##this is pathological\n",
    "w_0 = -10\n",
    "delta = 1.0\n",
    "cost = np.zeros(num_steps)\n",
    "weight_trajectory = np.zeros(num_steps)\n",
    "for ii in range(num_steps):\n",
    "    w_new = w_0-2*w_0*delta\n",
    "    w_0 = np.copy(w_new)\n",
    "    cost[ii] = w_new**2\n",
    "    weight_trajectory[ii] = w_new\n",
    "\n",
    "##plot cost as a function of weight trajectory\n",
    "plt.title('step size = %0.2f' %(delta))\n",
    "plt.ylabel('cost function value L(w)')\n",
    "plt.xlabel('weight at each iteration of grad. descent.')\n",
    "plt.plot(weight_trajectory,cost, '-o', label = '$w_{>0}$')\n",
    "plt.plot(weight_trajectory[0],cost[0], 'ro', label='$w_0$')\n",
    "plt.plot(w, w**2, 'k', label='$L(w)$')\n",
    "plt.legend(numpoints=1, loc = 'best')\n",
    "plt.xlim([-11,11])\n",
    "plt.ylim([0,110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##if delta is too big, you will see overflow errors\n",
    "w_0 = -10\n",
    "delta = 1.1\n",
    "cost = np.zeros(num_steps)\n",
    "weight_trajectory = np.zeros(num_steps)\n",
    "for ii in range(num_steps):\n",
    "    w_new = w_0-2*w_0*delta\n",
    "    w_0 = np.copy(w_new)\n",
    "    cost[ii] = w_new**2\n",
    "    weight_trajectory[ii] = w_new\n",
    "\n",
    "##plot cost as a function of weight trajectory\n",
    "plt.title('step size = %0.2f' %(delta))\n",
    "plt.ylabel('cost function value L(w)')\n",
    "plt.xlabel('weight at each iteration of grad. descent.')\n",
    "plt.plot(weight_trajectory,cost, '-o', label = '$w_{>0}$')\n",
    "plt.plot(weight_trajectory[0],cost[0], 'ro', label='$w_0$')\n",
    "plt.plot(w, w**2, 'k', label='$L(w)$')\n",
    "plt.legend(numpoints=1, loc = 'best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Generalize the above code to two dimensions, where cost function is defined:\n",
    "\n",
    "$L(\\mathbf{w}) = \\mathbf{w} \\cdot \\mathbf{w} $\n",
    "\n",
    "What is the derivative ? \n",
    "Draw the cost function trajectory, as above. Find a good step-size for this cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
