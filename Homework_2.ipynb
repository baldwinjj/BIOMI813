{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Linearity and depth\n",
    "Prove that a network composed of two linear fully connected layers (i.e. layer that perform the affine transformation $x_{i+1} = W_ix_i+b_i$) is equivalent to a single linear fully connected layer. Note that, by using recursivity, one could show that this property holds for network of any depth if they are solely composed of such layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Accuracies\n",
    "Explain in your own words what the training and validation accuracy are. Give and interpret one situation in which the validation accuracy may be greater than the training accuracy. Give and interpret one situation where the validation accuracy may be less than the training accuracy. What does it tell us about our model? What does it tell us about our dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: Multilayer perceptron\n",
    "\"model_2\" in lasagne_MLP_classifier_example.ipynb is composed of a single layer with N nodes and a nonlinearity followed by a linear classifier. Define a similar \"model_3\" multi-layer perceptron networks with 2 consecutive layers of N/2 output nodes each followed by rectifier nonlinearities (again with a final linear classifier on top), train it, and record your observation (final training and validation accuracy, number of iteration you trained it, etc.). How do the \"model_2\" and \"model_3\" networks compare? How many parameters do each of these network contain? Briefly explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: Convolutional network\n",
    "One type of transformation that has gained prominence in neural network architecture is the convolutional layer. The reason for this is that the convolution operation possess a property called \"equivariance\", which is closely related to spatial invariance. Equivariance means \"vary in the same way as\". Coupled with maxpooling, which means taking the maximum value in a certain region, the resulting activation often shows spatial invariance over small position changes in an image.\n",
    "\n",
    "A convolutional layer consists of a small \"local\" dense network (the kernel) which is repeated over different spatial locations. The kernel, which consists of all the learnable parameters, is the same regardless of the position. In lasagne, the convolutional layer is defined as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 5103)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'incoming' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e2f51045d91f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2DLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mincoming\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_filters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonlinearity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'incoming' is not defined"
     ]
    }
   ],
   "source": [
    "import lasagne.layers as L\n",
    "L.Conv2DLayer(incoming, num_filters, filter_size, stride, pad, W, b, nonlinearity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \"incoming\" is the layer below, \"num_filters\" is the output dimension of the convolutional kernel (the total number of spatial convolution performed is the feature dimension of the input times this number (of features) of the output), \"filter_size\" is the size of the kernel window, \"stride\" is the step size used when sliding this kernel window across the input, \"pad\" is some extra dimension applied to the input, \"W\" and \"b\" are the kernel parameters to be initialized and \"nonlinearity\" is an optional nonlinearity applied after the convolution operation.\n",
    "\n",
    "A pooling layer specify a set of region whose positions and size in a certain feature map are defined by \"pool_size\" and \"stride\", into each of which the maximum value is calculated. In lasagne, max pooling is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-93975965a4d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPool2DLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mincoming\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'L' is not defined"
     ]
    }
   ],
   "source": [
    "L.MaxPool2DLayer(incoming, pool_size, stride) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let \"XcYsZ\" indicate a (c)onvolutional layer for \"X\" output features with kernel size \"Y\" and (s)tride \"Z\", and let \"Xf\" indicate a fully connected layer with \"X\" output features. Note that we don't need to specify the input feature size because it is directly calculable in terms of all preceding layers and the network input. Furthermore, let's use a rectifier nonlinearity everywhere except the last layer where we will use no nonlinearity. A pooling layer is indicated as \"pXsY\" for a (p)ooling size \"X\" and stride \"Y\".\n",
    "\n",
    "With this in mind, construct the following network: \"input\" -> 64c7s3 -> 128c3s1 -> p2s1 -> 512f -> 10f. Use a \"W=I.Normal(0.02)\" and \n",
    "\n",
    "a) How many parameters are contained in this network? Briefly explain your reasoning.\n",
    "\n",
    "b) Use the framework we have developped (or any other framework in which you feel more confortable, as long as it is at least equality expressive) to train this network on the MNIST dataset and record your results. How do they compare with the MLP networks of Exercise 3?\n",
    "\n",
    "Note: The weight initialization matters. Try using random normal noise with low variance like \"I.Normal(0.02)\" or start from homogeneous zero values with \"I.Constant(0.)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
