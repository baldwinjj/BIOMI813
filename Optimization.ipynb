{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization \n",
    "\n",
    "Here we consider simple quadratic cost functions, examine their derivatives, and use gradient descent to find their minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic cost function, one dimension\n",
    "\n",
    "Here we have a cost function $L(w)$ in one dimension defined like:\n",
    "\n",
    "$L(w) = w^2$\n",
    "\n",
    "To find the bottom of this cost function, we start at any value of $w$ and then follow the derivative of the cost function for a small step-size $\\Delta$. The derivative looks like this:\n",
    "\n",
    "$\\frac{dL(w)}{dw} = 2w$\n",
    "\n",
    "This gives us the equation of the line that is tangent to the cost function at each point. We'll use \"quiver\" to draw segments (pink) of the tangent line at various points on $L(w)$. The length of the line segment is governed by $\\Delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##define step size and range of function evaluation\n",
    "delta = 0.9\n",
    "w = np.linspace(-10,10,num=100)\n",
    "\n",
    "##plot the cost function L(w)\n",
    "plt.plot(w, w**2)\n",
    "\n",
    "##plot a vector along the line defined by the derivative of L(w) for some values of w.\n",
    "x_color = np.array([.8,.3,.6])\n",
    "for ii in w[0:-1:5]:\n",
    "    plt.quiver(ii,ii**2,delta,2*ii*delta,angles='xy', scale_units ='xy',scale=1,pivot='middle',headwidth=0,color=x_color)\n",
    "plt.ylim([-5,100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try to reach the bottom of the cost function starting at some initial value $w_0$ and then travelling small distances in the direction of the derivative at each step. This is called \"gradient descent\". We'll see how important step size for determining how quickly you reach the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_0 = 10\n",
    "num_steps = 1000\n",
    "\n",
    "##let's try a small step-size first\n",
    "delta = .01\n",
    "cost = np.zeros(num_steps)\n",
    "weight_trajectory = np.zeros(num_steps)\n",
    "for ii in range(num_steps):\n",
    "    w_new = w_0-2*w_0*delta\n",
    "    w_0 = np.copy(w_new)\n",
    "    cost[ii] = w_new**2\n",
    "    weight_trajectory[ii] = w_new\n",
    "\n",
    "##plot cost as a function of weight trajectory\n",
    "plt.title('step size = %0.2f' %(delta))\n",
    "plt.ylabel('cost function value')\n",
    "plt.xlabel('weight at each iteration of grad. descent.')\n",
    "plt.plot(weight_trajectory, weight_trajectory**2, '-o')\n",
    "plt.plot(weight_trajectory[0],weight_trajectory[0]**2, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##now a larger delta\n",
    "w_0 = -10\n",
    "delta = .9\n",
    "cost = np.zeros(num_steps)\n",
    "weight_trajectory = np.zeros(num_steps)\n",
    "for ii in range(num_steps):\n",
    "    w_new = w_0-2*w_0*delta\n",
    "    w_0 = np.copy(w_new)\n",
    "    cost[ii] = w_new**2\n",
    "    weight_trajectory[ii] = w_new\n",
    "\n",
    "##plot cost as a function of weight trajectory\n",
    "plt.title('step size = %0.2f' %(delta))\n",
    "plt.ylabel('cost function value L(w)')\n",
    "plt.xlabel('weight at each iteration of grad. descent.')\n",
    "plt.plot(weight_trajectory,weight_trajectory**2, '-o')\n",
    "plt.plot(weight_trajectory[0],weight_trajectory[0]**2, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight_trajectory[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##this is pathological\n",
    "w_0 = -10\n",
    "delta = 1.0\n",
    "cost = np.zeros(num_steps)\n",
    "weight_trajectory = np.zeros(num_steps)\n",
    "for ii in range(num_steps):\n",
    "    w_new = w_0-2*w_0*delta\n",
    "    w_0 = np.copy(w_new)\n",
    "    cost[ii] = w_new**2\n",
    "    weight_trajectory[ii] = w_new\n",
    "\n",
    "##plot cost as a function of weight trajectory\n",
    "plt.title('step size = %0.2f' %(delta))\n",
    "plt.ylabel('cost function value')\n",
    "plt.xlabel('weight at each iteration of grad. descent.')\n",
    "plt.plot(weight_trajectory, weight_trajectory**2, '-o')\n",
    "plt.plot(weight_trajectory[0],weight_trajectory[0]**2, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##if delta is too big, you will see overflow errors\n",
    "w_0 = -10\n",
    "delta = 1.2\n",
    "cost = np.zeros(num_steps)\n",
    "weight_trajectory = np.zeros(num_steps)\n",
    "for ii in range(num_steps):\n",
    "    w_new = w_0-2*w_0*delta\n",
    "    w_0 = np.copy(w_new)\n",
    "    cost[ii] = w_new**2\n",
    "    weight_trajectory[ii] = w_new\n",
    "\n",
    "##plot cost as a function of weight trajectory\n",
    "plt.title('step size = %0.2f' %(delta))\n",
    "plt.ylabel('cost function value')\n",
    "plt.xlabel('weight at each iteration of grad. descent.')\n",
    "plt.plot(weight_trajectory, weight_trajectory**2, '-o')\n",
    "plt.plot(weight_trajectory[0],weight_trajectory[0]**2, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Generalize the above code to two dimensions, where cost function is defined:\n",
    "\n",
    "$L(\\mathbf{w}) = \\mathbf{w} \\cdot \\mathbf{w} $\n",
    "\n",
    "What is the derivative ? \n",
    "Draw the cost function trajectory, as above. Find a good step-size for this cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
