{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from loadMNIST import LoadMNIST\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An optimal linear classifier for multiple classes\n",
    "In this notebook, we'll build a classifer to discriminate between all of the digits in the MNIST database.\n",
    "\n",
    "Instead of sticking to a 2D feature space that allows us to hand-code an excellent classifier, we'll use a $D \\geq K$ feature space, where $D$ is the dimension of the feature space and $K=10$ is the number of class labels.\n",
    "\n",
    "This notebook will follow the same outline as the DIY classifier notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data\n",
    "We'll use the MNIST dataset for this exercise. First we'll load up the images and normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##load all MNIST images\n",
    "\n",
    "mnist_trn_img = '/mnt/fast/MNIST/train-images-idx3-ubyte'\n",
    "mnist_trn_lab = '/mnt/fast/MNIST/train-labels-idx1-ubyte'\n",
    "mnist_val_img = '/mnt/fast/MNIST/t10k-images-idx3-ubyte'\n",
    "mnist_val_lab = '/mnt/fast/MNIST/t10k-labels-idx1-ubyte'\n",
    "\n",
    "mnist_trn_data = LoadMNIST(mnist_trn_img).astype('float32')\n",
    "mnist_val_data = LoadMNIST(mnist_val_img).astype('float32')\n",
    "mnist_trn_label = LoadMNIST(mnist_trn_lab)\n",
    "mnist_val_label = LoadMNIST(mnist_val_lab)\n",
    "\n",
    "K = 10 ##number of classes\n",
    "n_trn_images = mnist_trn_data.shape[0]\n",
    "n_val_images = mnist_val_data.shape[0]\n",
    "pixel_res = mnist_trn_data.shape[1]\n",
    "\n",
    "print 'there are %d images in the training set' %(n_trn_images)\n",
    "print 'there are %d images in the validation set' %(n_val_images)\n",
    "\n",
    "##these will be useful later on\n",
    "trn_img_idx = np.arange(n_trn_images).astype('int')\n",
    "val_img_idx = np.arange(n_val_images).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##sanity check\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "for k in range(K):\n",
    "    plt.subplot(1,K,k+1)\n",
    "    idx = trn_img_idx[mnist_trn_label==k][0]\n",
    "    plt.imshow(mnist_trn_data[idx],cmap='gray')\n",
    "    plt.title('%d' %(k))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally a good idea to normalize the input data, so that feature space is not dominated by the \"brightness\" of images, but rather by their shape. To do this, we first subtract the mean from each image, then divide each image by it's magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##normalize images so that each \n",
    "##note the use of np.newaxis to take advantage of broadcasting rules!\n",
    "for arr in [mnist_trn_data, mnist_val_data]:\n",
    "    arr -= np.mean(arr,axis=(1,2))[:,np.newaxis,np.newaxis]  \n",
    "    arr /= np.sqrt(np.sum(arr**2,axis=(1,2)))[:,np.newaxis,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(mnist_trn_data,axis=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The feature space\n",
    "We'll construct a simple linear transform that maps each MNIST image into a point in a K-dimensional feature space.\n",
    "\n",
    "The axes of this feature space will correspond to randomly selected examples of each digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select two images from the training data to be \"templates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_idx = np.zeros(K).astype('int')\n",
    "templates = np.zeros((K,pixel_res,pixel_res))\n",
    "for k in range(K):\n",
    "    temp_idx[k] = np.random.permutation(trn_img_idx[mnist_trn_label==k])[0]\n",
    "    templates[k] = mnist_trn_data[temp_idx[k]]\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "##view your templates\n",
    "for k in range(K):\n",
    "    plt.subplot(1,K,k+1)\n",
    "    plt.imshow(templates[k],cmap='gray')\n",
    "    plt.title('template for %d' %(k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{t}_i$ be the template for digit $i$. Let $\\mathbf{s}$ be an image in the training set. Map all training data images into $D=K$-dimensional feature space defined\n",
    "\n",
    "$\\phi(\\mathbf{s}) = (\\mathbf{s} \\cdot \\mathbf{t_0}, \\ldots , \\mathbf{s} \\cdot \\mathbf{t_K})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##first, we reshape the templates and stack them into a matrix\n",
    "templates = templates.reshape((K,pixel_res**2)).T\n",
    "print templates.shape\n",
    "\n",
    "# ##next we project each of the images onto the templates\n",
    "##in what follows, we'll want samples along the columns, and feature dimensions along the rows.\n",
    "##so we'll take a transpose here\n",
    "trn_features = mnist_trn_data.reshape((n_trn_images,pixel_res**2)).dot(templates).T\n",
    "print trn_features.shape\n",
    "\n",
    "val_features = mnist_val_data.reshape((n_val_images,pixel_res**2)).dot(templates).T\n",
    "print val_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##for sanity, view the projections of into the 2D plane defined by the i^th and j^th templates\n",
    "one_color = np.array([.8,.3,.6])  ##pinkish\n",
    "i_temp = 7\n",
    "j_temp = 1\n",
    "\n",
    "plt.scatter(trn_features[i_temp,:], trn_features[j_temp,:],c=one_color,label='%d/%d feature plane' %(i_temp,j_temp), alpha = 1)\n",
    "plt.legend()\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlabel('projection onto %d -template' %(i_temp))\n",
    "plt.ylabel('projection onto %d -template' %(j_temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different projections have different 2D distributions. Some pairs of templates seem to pull the data points apart; some seem to smoosh them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an optimal multi-class classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we discussed a the multi-class discriminant function:\n",
    "\n",
    "$\\mathbf{y}(\\mathbf{x}) = W\\mathbf{x}$,\n",
    "\n",
    "where the $K \\times D$ weight matrix $W$ is defined:\n",
    "\n",
    "$W = \\begin{bmatrix} -&\\mathbf{w}^T_1&-\\\\ \\quad&\\ldots&\\quad \\\\ -&\\mathbf{w}^T_K&- \\end{bmatrix}$\n",
    "\n",
    "and $\\mathbf{w}^T_k = \\left (b_k, w_1, \\ldots, w_{D-1} \\right ) $\n",
    "\n",
    "and $\\mathbf{x} = \\begin{bmatrix} 1\\\\x_1\\\\ \\vdots\\\\ x_{D-1} \\end{bmatrix}$\n",
    "\n",
    "Then the class label $c_i$ for a single data point $\\mathbf{x}_i$ is just:\n",
    "\n",
    "$c_i = \\operatorname{argmax} \\mathbf{y}(\\mathbf{x}_i)$\n",
    "\n",
    "Note that in our feature space notation, $\\mathbf{x}_i = \\phi(\\mathbf{s}_i)$.\n",
    "\n",
    "Our goal will be to find the optimal weight matrix $W^*$. We'll define \"optimal\" as the weight matrix that minimizes sum of distances between the discriminator output $\\mathbf{y}(\\mathbf{x}_i)$ and a corresponding target vector $\\mathbf{t}_i$ that encodes the $K$ clases as a $K$-bit binary string. So for example, if $K=4$ and $c_i=3$ then\n",
    "\n",
    "$\\mathbf{t}_i = \\begin{bmatrix} 0\\\\0\\\\1\\\\0  \\end{bmatrix}$\n",
    "\n",
    "Assuming we have $N$ data samples, the sum of squared distances cost function is then\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    " L(W) = & \\sum_{i=1}^{i=N}\\left | \\left | \\mathbf{y}(\\mathbf{x}_i) - \\mathbf{t}_i \\right | \\right |^2 \\\\\n",
    "      = & \\sum_{i=1}^{i=N}\\left | \\left | W\\mathbf{x}_i - \\mathbf{t}_i \\right | \\right |^2 \n",
    " \\end{split}\n",
    " \\end{equation}\n",
    "$\n",
    "\n",
    "and \n",
    "\n",
    "$W^* = \\operatorname{argmin} L(W)$\n",
    "\n",
    "As we discussed in class, we'll estimate $W^*$ by making an initial guess about it's value, and then performing gradient descent on the cost function until we reach the (a) minumum:\n",
    "\n",
    "$W_{t+1} = W_t - \\Delta \\frac{dL(W)}{dW}$\n",
    "\n",
    "where $\\Delta$ is the (scalar) step size. In this case, the gradient has a simple expression:\n",
    "\n",
    "$\\frac{dL(W)}{dW} = \\sum_{i=1}^{i=N}2\\left (W\\mathbf{x}_i - \\mathbf{t}_i \\right )\\mathbf{x}_i^T$\n",
    "\n",
    "We can sanity-check the expression of the gradient by making sure it that dimensions of $W$ (which is a $K \\times D$ matrix). To do this we write out the shape of each term in the expression and get\n",
    "\n",
    "$\\left ( \\left (K \\times D \\right ) \\left (D \\times 1 \\right ) - \\left (K \\times 1 \\right ) \\right) \\left ( 1 \\times D \\right ) = \\left (K \\times 1 \\right ) \\left ( 1 \\times D \\right) = \\left (K \\times D \\right ) $\n",
    "\n",
    "Now we'll write a multi-class discriminant function, a function to compute $L(W)$, and a gradient function. Then we'll iterate out the equation above until some time-step $t^*$ such that $L(W_{t^*}) - L(W_{t^*-1})$ is below some threshold; i.e., we'll stop when the cost stops decreasing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##a basically gutless discriminant function\n",
    "def discriminant_function(X,W):\n",
    "    '''\n",
    "    discriminant_function(X,W)\n",
    "    X = D x N matrix of D x 1 of data columns. Note: We assume the top row of \"X\" is all 1's !!\n",
    "    W = K x D weight matrix\n",
    "    returns y(x) = Wx which has dimensions K x N\n",
    "    '''\n",
    "    return W.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function(T, X,W):\n",
    "    '''\n",
    "    cost_function(T,X,W)\n",
    "    T ~ K x N matrix of class labels\n",
    "    X ~ D x N matrix of D x 1 data columns\n",
    "    W ~ K x D weight matrix\n",
    "    returns sum of squared lengths of the difference vectors Wx-t\n",
    "    '''\n",
    "    diff = discriminant_function(X,W) - T ##K x N\n",
    "    return np.sum(np.sum(diff**2,axis=0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient(T, X, W):\n",
    "    '''\n",
    "    T ~ K x N matrix of class labels\n",
    "    X ~ D x N matrix of D x 1 data columns\n",
    "    W ~ K x D weight matrix\n",
    "    returns the K x D gradient of cost_function with respect to W\n",
    "    '''\n",
    "    diff = discriminant_function(X,W) - T  ##K x N\n",
    "    return diff.dot(X.T)  ##(K x N) (N x D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance(T,X,W):\n",
    "    N = T.shape[1]\n",
    "    trial_dx = range(N)\n",
    "    y = discriminant_function(X,W)\n",
    "    class_labels = np.argmax(y,axis=0)\n",
    "    num_correct = np.sum(T[list(class_labels),trial_dx])\n",
    "    prc_correct = num_correct/float(N)\n",
    "    print 'percent correct: %0.2f' %(prc_correct)\n",
    "    return prc_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent loop\n",
    "Now we implement a gradient descent loop on the template-based feature representation we established above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must first format the input/output data to align with the equations above. This means adding \"1's\" to the input data, and formatting the class labels in the $1-$of$-K$ format we describe above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##encode the dimensions of the problem using the notation we've developed above\n",
    "D = K+1 ##the +1 accounts for the bias term\n",
    "N_trn = n_trn_images\n",
    "N_val = n_val_images\n",
    "\n",
    "##prepare the feature data by adding appending a 1 to each of the data samples\n",
    "X_trn = np.concatenate([np.ones((1,N_trn)),trn_features])\n",
    "print X_trn.shape\n",
    "X_val = np.concatenate([np.ones((1,N_val)),val_features])\n",
    "print X_val.shape\n",
    "\n",
    "##prepare the target data (the class labels)\n",
    "##Note: we convert the indices from arrays to lists to bypass numpy's broadcasting rules, which can complicate\n",
    "##integer-based indexing.\n",
    "T_trn = np.zeros((K,N_trn))\n",
    "T_trn[list(mnist_trn_label),list(trn_img_idx)] = 1\n",
    "##sanity check the training data\n",
    "print T_trn[:,10], mnist_trn_label[10]\n",
    "\n",
    "T_val = np.zeros((K,N_val))\n",
    "T_val[list(mnist_val_label),list(val_img_idx)] = 1\n",
    "##sanity check the validation data\n",
    "print T_val[:,10], mnist_val_label[10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement the actual training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##random starting point\n",
    "W_new = np.random.randn(K,D)\n",
    "\n",
    "##calculate initial performance\n",
    "print 'Starting performance'\n",
    "P_new = performance(T_trn,X_trn,W_new)\n",
    "print 'chance is: %0.2f' %(1./K)\n",
    "\n",
    "##step-size\n",
    "delta = 0.01\n",
    "\n",
    "##stopping criterion: stop when difference between successive costs less than this number\n",
    "l_stop = 10e-4\n",
    "\n",
    "##also, to keep from running forever, we put in a back stop\n",
    "max_iters = 10000\n",
    "\n",
    "##step size\n",
    "delta = 0.00001\n",
    "\n",
    "##we'll want output during training, but probably not on every step\n",
    "report_every = 100\n",
    "\n",
    "##calculate initial cost\n",
    "L_old = np.inf  ##this is a trick. \"inf - x\" is never less than anything. \n",
    "L_new = cost_function(T_trn,X_trn,W_new)\n",
    "print 'cost: %0.3f' %(L_new)\n",
    "\n",
    "\n",
    "##train that model!\n",
    "iteration = 0\n",
    "while (iteration <= max_iters) & ((L_old - L_new) > l_stop):\n",
    "    W_grad = gradient(T_trn,X_trn,W_new)\n",
    "    W_new -= W_grad*delta\n",
    "    L_old = np.copy(L_new)\n",
    "    L_new = cost_function(T_trn,X_trn,W_new)\n",
    "    if not np.mod(iteration,report_every):\n",
    "        print 'iteration: %d, cost: %0.3f' %(iteration,L_new)\n",
    "    iteration += 1\n",
    "\n",
    "##calculate initial performance\n",
    "print 'Final performance'\n",
    "P_new = performance(T_trn,X_trn,W_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation performance\n",
    "So far, we've tested the performance on the training set. But to really know how our model is doing, we need to test it on a novel data set that the model was optimized on. This is known as the \"validation\" data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "##calculate initial performance\n",
    "print 'Performance on validation set'\n",
    "P_new = performance(T_val,X_val,W_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Exercise\n",
    "This validation peformance is *much* better than chance, but is still pretty far from perfect. One natural question to ask is: what digits does our classifier most often confuse? We can answer this question by calculating a \"confusion matrix\". Each element of this symmetric matrix gives the 2-way discrimination peformance for each pair of digits. By summing along the diagonal of the confusion matrix we obtain the total validation performance.\n",
    "\n",
    "Write a function to calculate the confusion matrix. For the simple linear classifier we just trained, what digits are most difficult to discriminate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting and regularization\n",
    "So far, we've been working with data in the $N >> D$ regime. This means that the number of training samples ($N$) far exceeds the number of dimensions of the feature space ($D$). However, in practice we very often find ourselves in the $N << D$ regime, where there are fewer data samples than there are dimensions. In this regime, we will encounter \"overfitting\", in which the performance evaluated on the training set is much better than the performance on the validation set. This means that the classifier has learned something very particular about the samples in training data, as opposed to something general about the classes themselves. Below, see what happens when train a system in which $N=9, D = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##random starting point\n",
    "W_new = np.random.randn(K,D)\n",
    "\n",
    "##subsample the training set so N < K\n",
    "n_trn_sub = 9\n",
    "trn_sub_idx = np.random.randint(0,N_trn,size=n_trn_sub)\n",
    "X_trn_sub = X_trn[:,trn_sub_idx]\n",
    "T_trn_sub = T_trn[:, trn_sub_idx]\n",
    "\n",
    "\n",
    "##calculate initial performance\n",
    "print 'Starting performance'\n",
    "P_new = performance(T_trn_sub,X_trn_sub,W_new)\n",
    "print 'chance is: %0.2f' %(1./K)\n",
    "\n",
    "##step-size\n",
    "delta = 0.01\n",
    "\n",
    "##stopping criterion: stop when difference between successive costs less than this number\n",
    "l_stop = 10e-7\n",
    "\n",
    "##also, to keep from running forever, we put in a back stop\n",
    "max_iters = 100000\n",
    "\n",
    "##step size\n",
    "delta = 0.00001\n",
    "\n",
    "##we'll want output during training, but probably not on every step\n",
    "report_every = 100\n",
    "\n",
    "##calculate initial cost\n",
    "L_old = np.inf  ##this is a trick. \"inf - x\" is never less than anything. \n",
    "L_new = cost_function(T_trn_sub,X_trn_sub,W_new)\n",
    "print 'cost: %0.3f' %(L_new)\n",
    "\n",
    "\n",
    "##train that model!\n",
    "iteration = 0\n",
    "while (iteration <= max_iters) & ((L_old - L_new) > l_stop):\n",
    "    W_grad = gradient(T_trn_sub,X_trn_sub,W_new)\n",
    "    W_new -= W_grad*delta\n",
    "    L_old = np.copy(L_new)\n",
    "    L_new = cost_function(T_trn_sub,X_trn_sub,W_new)\n",
    "    if not np.mod(iteration,report_every):\n",
    "        print 'iteration: %d, cost: %0.3f' %(iteration,L_new)\n",
    "    iteration += 1\n",
    "\n",
    "##calculate final performance on training set\n",
    "print 'Final performance on training set'\n",
    "P_new = performance(T_trn_sub,X_trn_sub,W_new)\n",
    "\n",
    "##calculate initial performance\n",
    "print 'Performance on validation set'\n",
    "P_new = performance(T_val,X_val,W_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execercise\n",
    "One way that we might improve our classifier is to use a feature space with more dimensions. Currenly, our feature space relies upon the similarity between many thousands of images and small set of templates. As we saw above, the distributions of these features do not seem to separate into distinct clusters. \n",
    "\n",
    "However, if we increase the dimensions of our model, we expose our model to the risk of overfitting.\n",
    "\n",
    "In this excerise we will learn how to control overfitting via *regularization*. Regularization is a term that refers to any added constraint to the training or added cost. In this case, we'll consider a simple and powerful form or regularization known as *early-stopping*. In early stopping, we subdivide our training data into a gradient set and an early stopping set. We use the gradient set to calculate the gradient, but we use the early-stopping set to calculate the cost. We will stop training when the cost evaluated on the early stopping set begins to rise. That is the point at which our model is learning something about the gradient set that does not generalize to the rest of the samples.\n",
    "\n",
    "Build a series of classifiers that uses increasing numbers of images as templates. Start with using two images per digit, and then scale up from there. To make this excercise instructive, use only a subset of 100 randomly selected training images. First, train the classifier without early stopping.\n",
    "\n",
    "Without early stopping, for what number of features does the classifier begin to overfit?\n",
    "\n",
    "Now, train the classifier with early stopping? How many feature dimensions before we start to overfit? Is there an improvement in validation peformance as the number of feature increases?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
