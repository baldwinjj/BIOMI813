{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "Summing vectors in series to generate a trajectory through space\n",
    "\n",
    "Suppose $\\mathbf{x}_0 = (1,0)$. Write a $\\mathbf{for}$ loop that performs a series of vector addtions according to the following rule:\n",
    "\n",
    "$\\mathbf{x}_{t+1} = \\mathbf{x}_{t}+\\lambda \\mathbf{x}^{\\perp}_{t}$\n",
    "\n",
    "where $\\mathbf{x}^{\\perp}$ is perpendicular to $\\mathbf{x}$ and $\\lambda$ is called the \"step-size\".\n",
    "\n",
    "In two dimensions, to obtain $\\mathbf{x}^{\\perp}$ simply swap vector elements, then negate the first element:\n",
    "\n",
    "$\\mathbf{x}^{\\perp} = \\left (-x_2, x_1 \\right )$\n",
    "\n",
    "Start by iterating for 10,000 steps, with a step size of 0.2. What trajectory is traced out by this procedure? How will the tajectory change as the step size is reduced?\n",
    "\n",
    "$\\textit{Hint}$: your solution should look like this: https://www.youtube.com/watch?v=FdCy6MGOVfw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "Use quiver to plot $\\mathbf{x}$, $\\mathbf{y}$, and the vector $\\mathbf{z}=\\mathbf{y}\\dfrac{\\mathbf{x}\\cdot \\mathbf{y}}{\\lvert \\lvert \\mathbf{y} \\rvert \\rvert^2}$ for some arbitrary $2D$ vectors $\\mathbf{x}$ and $\\mathbf{y}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "Zeros and ones are pretty easy to distinguish because they have very different shapes. Build a hand-coded classifier for ones and sevens, or threes and eights. How much worse is the performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "In class we constructed a feature space by using a randomly selected image of \"1\" and \"0\" as templates. \n",
    "\n",
    "Using code in the notebook \"DIY_classifier\", try to improve the performance of your DIY classifier using a more principled selection of templates. \n",
    "\n",
    "*Note*: The strategy is completely up to you. You'll be graded for doing something novel and reasonable, not for improving performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "Generalize the functions \"discriminant_function\" and \"view_class\" to the multi-class setting. Use these functions to hand code a $K=3$ classifer (e.g., 1's, 0's, and 2's). In order to make visualization possible, use only two template images (of your chosing) to define the feature space (i.e., $D=2$).\n",
    "\n",
    "*Note*: Chances are you won't get very good performance in this $K > D$ setting. We might expect better performance when $K \\geq D$, but that will require new methods, which we'll cover next time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "Write a function to calculate the gradient for a toy-example cost function defined:\n",
    "\n",
    "$L(\\mathbf{w}) = \\mathbf{w} \\cdot \\mathbf{w} $, where $\\mathbf{w}$ is $2D$ weight vector.\n",
    "\n",
    "\n",
    "Write some code that descends to the bottom of this cost function. Plot the cost function trajectory in the $2D$ plane, just as we did for the $1D$ case in the \"Simple_gradient_descent_illustration\" notebook. Show what happens when the step-size is \"small enough\" and when it is \"too large\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n",
    "In the \"Optimal_multi_class\" notebook, we trained a linear classifer with using a $D=10$ dimensional template-based feature space, and $K=60,000$ training samples. In this setting, validation peformance was *much* better than chance, but still pretty far from perfect. One natural question to ask is: what digits does our classifier most often confuse? We can answer this question by calculating a \"confusion matrix\". Each element of this symmetric matrix gives the 2-way discrimination peformance for each pair of digits. By summing along the diagonal of the confusion matrix we obtain the total validation performance.\n",
    "\n",
    "Write a function to calculate the confusion matrix. For the simple linear classifier we trained in \"Optimal_multi_class\" notebook, what digits are most difficult to discriminate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execercise 8\n",
    "One way that we might improve our classifier is to use a feature space with more dimensions. Currenly, our feature space relies upon the similarity between many thousands of images and small set of templates. As we saw above, the distributions of these features do not seem to separate into distinct clusters. \n",
    "\n",
    "However, if we increase the dimensions of our model, we expose our model to the risk of overfitting.\n",
    "\n",
    "In this excerise we will learn how to control overfitting via *regularization*. Regularization is a term that refers to any added constraint to the training or added cost. In this case, we'll consider a simple and powerful form or regularization known as *early-stopping*. In early stopping, we subdivide our training data into a gradient set and an early stopping set. We use the gradient set to calculate the gradient, but we use the early-stopping set to calculate the cost. We will stop training when the cost evaluated on the early stopping set begins to rise. That is the point at which our model is learning something about the gradient set that does not generalize to the rest of the samples.\n",
    "\n",
    "Build a series of classifiers that uses increasing numbers of images as templates. Start with using two images per digit, and then scale up from there. To make this excercise instructive, use only a subset of 100 randomly selected training images. First, train the classifier without early stopping.\n",
    "\n",
    "Without early stopping, for what number of features does the classifier begin to overfit?\n",
    "\n",
    "Now, train the classifier with early stopping? How many feature dimensions before we start to overfit? Is there an improvement in validation peformance as the number of feature increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
